{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuoYZBqTCupT6216VqYskd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/Adversarial-Debiasing-Extended/blob/main/Adversarial_Debiasing_Extended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFWOnUus397R"
      },
      "outputs": [],
      "source": [
        "# Enhanced Adversarial Debiasing with Intersectionality, Validation Metrics, and Clean Dropout API\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import logging\n",
        "\n",
        "from aif360.algorithms import Transformer\n",
        "\n",
        "# Disable TensorFlow 2.x behavior\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class AdversarialDebiasing(Transformer):\n",
        "    def __init__(self,\n",
        "                 unprivileged_groups,\n",
        "                 privileged_groups,\n",
        "                 scope_name,\n",
        "                 sess,\n",
        "                 seed=None,\n",
        "                 adversary_loss_weight=0.1,\n",
        "                 num_epochs=50,\n",
        "                 batch_size=128,\n",
        "                 classifier_num_hidden_units=200,\n",
        "                 dropout=0.8,\n",
        "                 early_stopping_patience=5,\n",
        "                 validation_dataset=None,\n",
        "                 verbose=True,\n",
        "                 debias=True):\n",
        "\n",
        "        super().__init__(\n",
        "            unprivileged_groups=unprivileged_groups,\n",
        "            privileged_groups=privileged_groups)\n",
        "\n",
        "        self.scope_name = scope_name\n",
        "        self.seed = seed\n",
        "        self.sess = sess\n",
        "        self.adversary_loss_weight = adversary_loss_weight\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
        "        self.dropout = dropout\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.verbose = verbose\n",
        "        self.debias = debias\n",
        "\n",
        "        if self.verbose:\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(1, 9999, 4)\n",
        "\n",
        "    def _encode_protected_attributes(self, dataset):\n",
        "        indices = [dataset.feature_names.index(attr) for attr in dataset.protected_attribute_names]\n",
        "        values = dataset.features[:, indices]\n",
        "        tuples = [tuple(row) for row in values]\n",
        "        unique_combos = sorted(set(tuples))\n",
        "        self.combo_to_class = {combo: i for i, combo in enumerate(unique_combos)}\n",
        "        encoded = np.array([self.combo_to_class[t] for t in tuples], dtype=np.int32)\n",
        "        return encoded, len(unique_combos)\n",
        "\n",
        "    def _classifier_model(self, features, features_dim, keep_prob):\n",
        "        with tf.variable_scope(\"classifier_model\"):\n",
        "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
        "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
        "            b1 = tf.Variable(tf.zeros([self.classifier_num_hidden_units]))\n",
        "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
        "            h1 = tf.nn.dropout(h1, rate=1 - keep_prob, seed=self.seed2)\n",
        "\n",
        "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
        "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
        "            b2 = tf.Variable(tf.zeros([1]))\n",
        "            logits = tf.matmul(h1, W2) + b2\n",
        "            pred = tf.sigmoid(logits)\n",
        "        return pred, logits\n",
        "\n",
        "    def _adversary_model(self, pred_logits, true_labels, num_classes):\n",
        "        with tf.variable_scope(\"adversary_model\"):\n",
        "            s = tf.sigmoid(pred_logits)\n",
        "            input_concat = tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1)\n",
        "            W = tf.get_variable('W_adv', [input_concat.shape[1], num_classes],\n",
        "                                initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
        "            b = tf.Variable(tf.zeros([num_classes]))\n",
        "            logits = tf.matmul(input_concat, W) + b\n",
        "            preds = tf.nn.softmax(logits)\n",
        "        return preds, logits\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        if tf.executing_eagerly():\n",
        "            raise RuntimeError(\"AdversarialDebiasing does not work in eager execution mode.\")\n",
        "\n",
        "        temp_labels = dataset.labels.copy()\n",
        "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(), 0] = 1.0\n",
        "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(), 0] = 0.0\n",
        "\n",
        "        prot_attr_encoded, num_classes = self._encode_protected_attributes(dataset)\n",
        "        num_samples, self.features_dim = dataset.features.shape\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        with tf.variable_scope(self.scope_name):\n",
        "            self.features_ph = tf.placeholder(tf.float32, [None, self.features_dim])\n",
        "            self.true_labels_ph = tf.placeholder(tf.float32, [None, 1])\n",
        "            self.protected_ph = tf.placeholder(tf.int32, [None])\n",
        "            self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
        "            pred_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
        "\n",
        "            if self.debias:\n",
        "                adv_preds, adv_logits = self._adversary_model(pred_logits, self.true_labels_ph, num_classes)\n",
        "                adv_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.protected_ph, logits=adv_logits))\n",
        "\n",
        "            classifier_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=f\"{self.scope_name}/classifier_model\")\n",
        "            if self.debias:\n",
        "                adversary_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=f\"{self.scope_name}/adversary_model\")\n",
        "                adversary_opt = tf.train.AdamOptimizer(0.001)\n",
        "                adv_grads = {v: g for g, v in adversary_opt.compute_gradients(adv_loss, classifier_vars)}\n",
        "                normalize = lambda x: x / (tf.norm(x) + 1e-8)\n",
        "\n",
        "            classifier_opt = tf.train.AdamOptimizer(0.001)\n",
        "            classifier_grads = []\n",
        "            for grad, var in classifier_opt.compute_gradients(pred_loss, classifier_vars):\n",
        "                if self.debias:\n",
        "                    u = normalize(adv_grads[var])\n",
        "                    grad = grad - tf.reduce_sum(grad * u) * u - self.adversary_loss_weight * adv_grads[var]\n",
        "                classifier_grads.append((tf.clip_by_value(grad, -5.0, 5.0), var))\n",
        "\n",
        "            classifier_step = classifier_opt.apply_gradients(classifier_grads)\n",
        "            if self.debias:\n",
        "                with tf.control_dependencies([classifier_step]):\n",
        "                    adversary_step = adversary_opt.minimize(adv_loss, var_list=adversary_vars)\n",
        "\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            for epoch in range(self.num_epochs):\n",
        "                epoch_loss = 0\n",
        "                shuffled = np.random.permutation(num_samples)\n",
        "\n",
        "                for i in range(0, num_samples, self.batch_size):\n",
        "                    batch = shuffled[i:i + self.batch_size]\n",
        "                    feed = {\n",
        "                        self.features_ph: dataset.features[batch],\n",
        "                        self.true_labels_ph: temp_labels[batch].reshape(-1, 1),\n",
        "                        self.protected_ph: prot_attr_encoded[batch],\n",
        "                        self.keep_prob: self.dropout\n",
        "                    }\n",
        "                    if self.debias:\n",
        "                        _, _, l_cls, l_adv = self.sess.run([classifier_step, adversary_step, pred_loss, adv_loss], feed_dict=feed)\n",
        "                        epoch_loss += l_cls + l_adv\n",
        "                    else:\n",
        "                        _, l_cls = self.sess.run([classifier_step, pred_loss], feed_dict=feed)\n",
        "                        epoch_loss += l_cls\n",
        "\n",
        "                if self.verbose:\n",
        "                    logging.info(f\"Epoch {epoch}: Train Loss = {epoch_loss:.4f}\")\n",
        "\n",
        "                if self.validation_dataset:\n",
        "                    val_loss = self._evaluate_loss(self.validation_dataset)\n",
        "                    if self.verbose:\n",
        "                        logging.info(f\"Validation Loss = {val_loss:.4f}\")\n",
        "                    if val_loss < best_val_loss:\n",
        "                        best_val_loss = val_loss\n",
        "                        patience_counter = 0\n",
        "                    else:\n",
        "                        patience_counter += 1\n",
        "                        if patience_counter >= self.early_stopping_patience:\n",
        "                            logging.info(\"Early stopping triggered.\")\n",
        "                            break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _evaluate_loss(self, val_dataset):\n",
        "        temp_labels = val_dataset.labels.copy()\n",
        "        temp_labels[(val_dataset.labels == val_dataset.favorable_label).ravel(), 0] = 1.0\n",
        "        temp_labels[(val_dataset.labels == val_dataset.unfavorable_label).ravel(), 0] = 0.0\n",
        "\n",
        "        prot_attr_encoded, _ = self._encode_protected_attributes(val_dataset)\n",
        "\n",
        "        feed = {\n",
        "            self.features_ph: val_dataset.features,\n",
        "            self.true_labels_ph: temp_labels,\n",
        "            self.protected_ph: prot_attr_encoded,\n",
        "            self.keep_prob: 1.0\n",
        "        }\n",
        "\n",
        "        preds = self.sess.run(self.pred_labels, feed_dict=feed)\n",
        "        return np.mean(np.square(preds - temp_labels))\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        temp_labels = dataset.labels.copy()\n",
        "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(), 0] = 1.0\n",
        "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(), 0] = 0.0\n",
        "\n",
        "        num_samples = len(dataset.features)\n",
        "        pred_labels = []\n",
        "        for i in range(0, num_samples, self.batch_size):\n",
        "            batch_features = dataset.features[i:i + self.batch_size]\n",
        "            batch_labels = np.reshape(temp_labels[i:i + self.batch_size], [-1, 1])\n",
        "\n",
        "            feed_dict = {\n",
        "                self.features_ph: batch_features,\n",
        "                self.true_labels_ph: batch_labels,\n",
        "                self.keep_prob: 1.0\n",
        "            }\n",
        "\n",
        "            pred_labels += self.sess.run(self.pred_labels, feed_dict=feed_dict)[:, 0].tolist()\n",
        "\n",
        "        dataset_new = dataset.copy(deepcopy=True)\n",
        "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
        "        dataset_new.labels = (np.array(pred_labels) > 0.5).astype(np.float64).reshape(-1, 1)\n",
        "\n",
        "        dataset_new.labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
        "        dataset_new.labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
        "\n",
        "        return dataset_new"
      ]
    }
  ]
}