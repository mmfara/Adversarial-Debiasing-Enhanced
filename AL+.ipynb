{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+4MOD6g5nFteY/Tpp1TEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/Adversarial-Debiasing-Enhanced/blob/main/AL%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJACRx1fpiEm"
      },
      "outputs": [],
      "source": [
        "#New modified\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior()\n",
        "except ImportError as error:\n",
        "    from logging import warning\n",
        "    warning(\"{}: AdversarialDebiasing will be unavailable. To install, run:\\n\"\n",
        "            \"pip install 'aif360[AdversarialDebiasing]'\".format(error))\n",
        "\n",
        "from aif360.algorithms import Transformer\n",
        "\n",
        "class AdversarialDebiasing(Transformer):\n",
        "    def __init__(self,\n",
        "                 unprivileged_groups,\n",
        "                 privileged_groups,\n",
        "                 scope_name,\n",
        "                 sess,\n",
        "                 seed=None,\n",
        "                 adversary_loss_weight=0.1,\n",
        "                 num_epochs=50,\n",
        "                 batch_size=128,\n",
        "                 classifier_num_hidden_units=200,\n",
        "                 debias=True):\n",
        "\n",
        "        super(AdversarialDebiasing, self).__init__(\n",
        "            unprivileged_groups=unprivileged_groups,\n",
        "            privileged_groups=privileged_groups)\n",
        "\n",
        "        self.scope_name = scope_name\n",
        "        self.seed = seed\n",
        "\n",
        "        self.unprivileged_groups = unprivileged_groups\n",
        "        self.privileged_groups = privileged_groups\n",
        "\n",
        "        # Allow multiple unprivileged/privileged group values\n",
        "        protected_attrs = set()\n",
        "        for group in (self.unprivileged_groups + self.privileged_groups):\n",
        "            protected_attrs.update(group.keys())\n",
        "\n",
        "        if len(protected_attrs) != 1:\n",
        "            raise ValueError(\"Only one protected attribute can be handled at a time.\")\n",
        "\n",
        "        self.protected_attribute_name = list(protected_attrs)[0]\n",
        "\n",
        "        self.sess = sess\n",
        "        self.adversary_loss_weight = adversary_loss_weight\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
        "        self.debias = debias\n",
        "\n",
        "        self.features_dim = None\n",
        "        self.features_ph = None\n",
        "        self.protected_attributes_ph = None\n",
        "        self.true_labels_ph = None\n",
        "        self.pred_labels = None\n",
        "\n",
        "    def _classifier_model(self, features, features_dim, keep_prob):\n",
        "        with tf.variable_scope(\"classifier_model\"):\n",
        "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
        "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed1))\n",
        "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
        "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
        "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob, seed=self.seed2)\n",
        "\n",
        "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
        "                                 initializer=tf.initializers.glorot_uniform(seed=self.seed3))\n",
        "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
        "\n",
        "            pred_logit = tf.matmul(h1, W2) + b2\n",
        "            pred_label = tf.sigmoid(pred_logit)\n",
        "\n",
        "        return pred_label, pred_logit\n",
        "\n",
        "    def _adversary_model(self, pred_logits, true_labels, num_classes):\n",
        "        with tf.variable_scope(\"adversary_model\"):\n",
        "            s = tf.sigmoid(pred_logits)\n",
        "            concat_input = tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1)\n",
        "            input_dim = concat_input.shape[1]\n",
        "\n",
        "            W = tf.get_variable('W_adv', [input_dim, num_classes],\n",
        "                                initializer=tf.initializers.glorot_uniform(seed=self.seed4))\n",
        "            b = tf.Variable(tf.zeros(shape=[num_classes]), name='b_adv')\n",
        "\n",
        "            logits = tf.matmul(concat_input, W) + b\n",
        "            preds = tf.nn.softmax(logits)\n",
        "\n",
        "        return preds, logits\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        if tf.executing_eagerly():\n",
        "            raise RuntimeError(\"AdversarialDebiasing does not work in eager execution mode.\")\n",
        "\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        ii32 = np.iinfo(np.int32)\n",
        "        self.seed1, self.seed2, self.seed3, self.seed4 = np.random.randint(ii32.min, ii32.max, size=4)\n",
        "\n",
        "        temp_labels = dataset.labels.copy()\n",
        "        if not np.array_equal(np.unique(temp_labels), [0.0, 1.0]):\n",
        "            temp_labels[(dataset.labels == dataset.favorable_label).ravel(), 0] = 1.0\n",
        "            temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(), 0] = 0.0\n",
        "\n",
        "        if np.isnan(dataset.features).any() or np.isinf(dataset.features).any():\n",
        "            raise ValueError(\"Features contain NaN or Inf values. Please clean the data.\")\n",
        "\n",
        "        with tf.variable_scope(self.scope_name):\n",
        "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
        "\n",
        "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
        "            self.protected_attributes_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "            self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
        "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
        "\n",
        "            unique_prot_attr = np.unique(dataset.protected_attributes[:, dataset.protected_attribute_names.index(self.protected_attribute_name)])\n",
        "            num_classes = len(unique_prot_attr)\n",
        "\n",
        "            if self.debias:\n",
        "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(\n",
        "                    pred_logits, self.true_labels_ph, num_classes)\n",
        "                pred_protected_attributes_loss = tf.reduce_mean(\n",
        "                    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                        labels=self.protected_attributes_ph,\n",
        "                        logits=pred_protected_attributes_logits))\n",
        "\n",
        "            global_step = tf.Variable(0, trainable=False)\n",
        "            starter_learning_rate = 0.0001  # More stable\n",
        "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
        "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
        "            if self.debias:\n",
        "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "            classifier_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'classifier_model' in var.name]\n",
        "            if self.debias:\n",
        "                adversary_vars = [var for var in tf.trainable_variables(scope=self.scope_name) if 'adversary_model' in var.name]\n",
        "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss, var_list=classifier_vars)}\n",
        "\n",
        "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
        "\n",
        "            classifier_grads = []\n",
        "            for (grad, var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
        "                if self.debias:\n",
        "                    unit_adv_grad = normalize(adversary_grads[var])\n",
        "                    grad -= tf.reduce_sum(grad * unit_adv_grad) * unit_adv_grad\n",
        "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
        "                grad = tf.clip_by_value(grad, -5.0, 5.0)  # Gradient clipping\n",
        "                classifier_grads.append((grad, var))\n",
        "\n",
        "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
        "            if self.debias:\n",
        "                with tf.control_dependencies([classifier_minimizer]):\n",
        "                    adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars)\n",
        "\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "            self.sess.run(tf.local_variables_initializer())\n",
        "\n",
        "            for epoch in range(self.num_epochs):\n",
        "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
        "                for i in range(num_train_samples // self.batch_size):\n",
        "                    batch_ids = shuffled_ids[self.batch_size * i: self.batch_size * (i + 1)]\n",
        "                    batch_features = dataset.features[batch_ids]\n",
        "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1, 1])\n",
        "                    batch_protected_attributes = dataset.protected_attributes[batch_ids][:,\n",
        "                        dataset.protected_attribute_names.index(self.protected_attribute_name)]\n",
        "\n",
        "                    feed_dict = {\n",
        "                        self.features_ph: batch_features,\n",
        "                        self.true_labels_ph: batch_labels,\n",
        "                        self.protected_attributes_ph: batch_protected_attributes,\n",
        "                        self.keep_prob: 0.8\n",
        "                    }\n",
        "\n",
        "                    if self.debias:\n",
        "                        _, _, loss_cls, loss_adv = self.sess.run([\n",
        "                            classifier_minimizer, adversary_minimizer,\n",
        "                            pred_labels_loss, pred_protected_attributes_loss\n",
        "                        ], feed_dict=feed_dict)\n",
        "                        if i % 200 == 0:\n",
        "                            print(f\"epoch {epoch}; iter: {i}; classifier loss: {loss_cls}; adversarial loss: {loss_adv}\")\n",
        "                    else:\n",
        "                        _, loss_cls = self.sess.run([\n",
        "                            classifier_minimizer,\n",
        "                            pred_labels_loss\n",
        "                        ], feed_dict=feed_dict)\n",
        "                        if i % 200 == 0:\n",
        "                            print(f\"epoch {epoch}; iter: {i}; classifier loss: {loss_cls}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, dataset):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        num_test_samples, _ = np.shape(dataset.features)\n",
        "        samples_covered = 0\n",
        "        pred_labels = []\n",
        "\n",
        "        while samples_covered < num_test_samples:\n",
        "            start = samples_covered\n",
        "            end = min(start + self.batch_size, num_test_samples)\n",
        "            batch_ids = np.arange(start, end)\n",
        "            batch_features = dataset.features[batch_ids]\n",
        "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1, 1])\n",
        "\n",
        "            feed_dict = {\n",
        "                self.features_ph: batch_features,\n",
        "                self.true_labels_ph: batch_labels,\n",
        "                self.keep_prob: 1.0\n",
        "            }\n",
        "\n",
        "            pred_labels += self.sess.run(self.pred_labels, feed_dict=feed_dict)[:, 0].tolist()\n",
        "            samples_covered += len(batch_features)\n",
        "\n",
        "        dataset_new = dataset.copy(deepcopy=True)\n",
        "        dataset_new.scores = np.array(pred_labels, dtype=np.float64).reshape(-1, 1)\n",
        "        dataset_new.labels = (np.array(pred_labels) > 0.5).astype(np.float64).reshape(-1, 1)\n",
        "\n",
        "        temp_labels = dataset_new.labels.copy()\n",
        "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
        "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
        "\n",
        "        dataset_new.labels = temp_labels.copy()\n",
        "        return dataset_new\n"
      ]
    }
  ]
}